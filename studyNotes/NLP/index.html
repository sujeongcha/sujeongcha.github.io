---
layout: page
title: Study Nodes
---

<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Understanding BERT for NLP Starters</title></head><body><article id="9ab9f2e3-06c3-4d14-b44c-59021dd555bd" class="page sans"><header><div class="page-header-icon undefined"><span class="icon">1ï¸âƒ£</span></div><h1 class="page-title">Understanding BERT for NLP Starters</h1><table class="properties"><tbody><tr class="property-row property-row-date"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesDate"><path d="M10.8889,5.5 L3.11111,5.5 L3.11111,7.05556 L10.8889,7.05556 L10.8889,5.5 Z M12.4444,1.05556 L11.6667,1.05556 L11.6667,0 L10.1111,0 L10.1111,1.05556 L3.88889,1.05556 L3.88889,0 L2.33333,0 L2.33333,1.05556 L1.55556,1.05556 C0.692222,1.05556 0.00777777,1.75556 0.00777777,2.61111 L0,12.5 C0,13.3556 0.692222,14 1.55556,14 L12.4444,14 C13.3,14 14,13.3556 14,12.5 L14,2.61111 C14,1.75556 13.3,1.05556 12.4444,1.05556 Z M12.4444,12.5 L1.55556,12.5 L1.55556,3.94444 L12.4444,3.94444 L12.4444,12.5 Z M8.55556,8.61111 L3.11111,8.61111 L3.11111,10.1667 L8.55556,10.1667 L8.55556,8.61111 Z"></path></svg></span>Dates</th><td><time>@Jun 3, 2020 â†’ Jun 11, 2020</time></td></tr><tr class="property-row property-row-text"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesText"><path d="M7,4.56818 C7,4.29204 6.77614,4.06818 6.5,4.06818 L0.5,4.06818 C0.223858,4.06818 0,4.29204 0,4.56818 L0,5.61364 C0,5.88978 0.223858,6.11364 0.5,6.11364 L6.5,6.11364 C6.77614,6.11364 7,5.88978 7,5.61364 L7,4.56818 Z M0.5,1 C0.223858,1 0,1.223858 0,1.5 L0,2.54545 C0,2.8216 0.223858,3.04545 0.5,3.04545 L12.5,3.04545 C12.7761,3.04545 13,2.8216 13,2.54545 L13,1.5 C13,1.223858 12.7761,1 12.5,1 L0.5,1 Z M0,8.68182 C0,8.95796 0.223858,9.18182 0.5,9.18182 L11.5,9.18182 C11.7761,9.18182 12,8.95796 12,8.68182 L12,7.63636 C12,7.36022 11.7761,7.13636 11.5,7.13636 L0.5,7.13636 C0.223858,7.13636 0,7.36022 0,7.63636 L0,8.68182 Z M0,11.75 C0,12.0261 0.223858,12.25 0.5,12.25 L9.5,12.25 C9.77614,12.25 10,12.0261 10,11.75 L10,10.70455 C10,10.4284 9.77614,10.20455 9.5,10.20455 L0.5,10.20455 C0.223858,10.20455 0,10.4284 0,10.70455 L0,11.75 Z"></path></svg></span>Hashtag</th><td>Pre-training, BERT</td></tr><tr class="property-row property-row-url"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesUrl"><path d="M3.73333,3.86667 L7.46667,3.86667 C8.49613,3.86667 9.33333,4.70387 9.33333,5.73333 C9.33333,6.7628 8.49613,7.6 7.46667,7.6 L6.53333,7.6 C6.01813,7.6 5.6,8.0186 5.6,8.53333 C5.6,9.04807 6.01813,9.46667 6.53333,9.46667 L7.46667,9.46667 C9.5284,9.46667 11.2,7.79507 11.2,5.73333 C11.2,3.6716 9.5284,2 7.46667,2 L3.73333,2 C1.6716,2 0,3.6716 0,5.73333 C0,7.124 0.762067,8.33453 1.88953,8.97713 C1.87553,8.83107 1.86667,8.6836 1.86667,8.53333 C1.86667,7.92013 1.98753,7.33447 2.2036,6.7978 C1.99267,6.4954 1.86667,6.12953 1.86667,5.73333 C1.86667,4.70387 2.70387,3.86667 3.73333,3.86667 Z M12.1095,5.28907 C12.1231,5.4356 12.1333,5.58307 12.1333,5.73333 C12.1333,6.34607 12.0101,6.9294 11.7931,7.46513 C12.0059,7.768 12.1333,8.13573 12.1333,8.53333 C12.1333,9.5628 11.2961,10.4 10.2667,10.4 L6.53333,10.4 C5.50387,10.4 4.66667,9.5628 4.66667,8.53333 C4.66667,7.50387 5.50387,6.66667 6.53333,6.66667 L7.46667,6.66667 C7.98187,6.66667 8.4,6.24807 8.4,5.73333 C8.4,5.2186 7.98187,4.8 7.46667,4.8 L6.53333,4.8 C4.4716,4.8 2.8,6.4716 2.8,8.53333 C2.8,10.59507 4.4716,12.2667 6.53333,12.2667 L10.2667,12.2667 C12.3284,12.2667 14,10.59507 14,8.53333 C14,7.14267 13.2375,5.93167 12.1095,5.28907 Z"></path></svg></span>Property</th><td><a href="https://arxiv.org/pdf/1810.04805.pdf" class="url-value">https://arxiv.org/pdf/1810.04805.pdf</a></td></tr><tr class="property-row property-row-select"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesSelect"><path d="M7,13 C10.31348,13 13,10.31371 13,7 C13,3.68629 10.31348,1 7,1 C3.68652,1 1,3.68629 1,7 C1,10.31371 3.68652,13 7,13 Z M3.75098,5.32278 C3.64893,5.19142 3.74268,5 3.90869,5 L10.09131,5 C10.25732,5 10.35107,5.19142 10.24902,5.32278 L7.15771,9.29703 C7.07764,9.39998 6.92236,9.39998 6.84229,9.29703 L3.75098,5.32278 Z"></path></svg></span>Status</th><td><span class="selected-value select-value-color-green">Done ğŸ™Œ</span></td></tr></tbody></table></header><div class="page-body"><h2 id="f5590dd9-51b7-418e-af91-6dd76b02b8f8" class="">Word-Embeddings</h2><div id="ff67e192-52a4-4391-89db-f70ce7a9fc12" class="column-list"><div id="a50bb45f-aaaa-4258-baa7-82da0d2034c6" style="width:43.75%" class="column"><ul id="d9b97a79-71a9-496d-add4-6d53e08a955f" class="bulleted-list"><li><mark class="highlight-yellow"><em><strong>Continuous Bag of Words Model (CBOW)</strong></em></mark>: the distributed representations of context (or surrounding words) are combined toÂ predict the word in the middle</li></ul><ul id="3d852723-6d3a-4236-90d6-0ab533153912" class="bulleted-list"><li><mark class="highlight-yellow"><em><strong>Skip-gram</strong></em></mark>: the distributed representation of the input word is used to predict the context.</li></ul></div><div id="8890f0a5-fb12-417e-9a9a-fbbc6e707022" style="width:56.25%" class="column"><figure id="f940e4a6-55a9-4ff1-a1d2-12eb9ada87fe" class="image"><a href="Understanding%20BERT%20for%20NLP%20Starters%20f940e4a655a94ff1a1d212eb9ada87fe/Untitled.png"><img style="width:480px" src="Understanding%20BERT%20for%20NLP%20Starters%20f940e4a655a94ff1a1d212eb9ada87fe/Untitled.png"/></a></figure></div></div><p id="fd5e8207-e5c6-47f1-bde5-dbae427b7090" class="">ğŸŒ<mark class="highlight-gray"> </mark><mark class="highlight-gray"><em>Reference: </em></mark><mark class="highlight-gray"><em><a href="https://towardsdatascience.com/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314">NLP 101: Word2Vec - Skip-gram and CBOW, Towards Data Science</a></em></mark></p><h2 id="1f94461b-ef95-4468-98e5-b410c3f93299" class="">Pre-Training</h2><ul id="a2337931-2c43-4041-8ec3-d2d2aff158f3" class="bulleted-list"><li>Training a model with one task to help it form parameters that can be used in other tasks</li></ul><ul id="ac39cb1a-fd9c-4d17-bab5-9a90cd4a54ff" class="bulleted-list"><li>(in NLP) equivalent to building a language representation model</li></ul><div id="08acfddf-9c0d-4fe5-be97-e3300194e8e1" class="collection-content"><h4 class="collection-title">NLP Pre-Training Technologies</h4><table class="collection-content"><thead><tr><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesTitle"><path d="M7.73943662,8.6971831 C7.77640845,8.7834507 7.81338028,8.8943662 7.81338028,9.00528169 C7.81338028,9.49823944 7.40669014,9.89260563 6.91373239,9.89260563 C6.53169014,9.89260563 6.19894366,9.64612676 6.08802817,9.30105634 L5.75528169,8.33978873 L2.05809859,8.33978873 L1.72535211,9.30105634 C1.61443662,9.64612676 1.2693662,9.89260563 0.887323944,9.89260563 C0.394366197,9.89260563 0,9.49823944 0,9.00528169 C0,8.8943662 0.0246478873,8.7834507 0.0616197183,8.6971831 L2.46478873,2.48591549 C2.68661972,1.90669014 3.24119718,1.5 3.90669014,1.5 C4.55985915,1.5 5.12676056,1.90669014 5.34859155,2.48591549 L7.73943662,8.6971831 Z M2.60035211,6.82394366 L5.21302817,6.82394366 L3.90669014,3.10211268 L2.60035211,6.82394366 Z M11.3996479,3.70598592 C12.7552817,3.70598592 14,4.24823944 14,5.96126761 L14,9.07922535 C14,9.52288732 13.6549296,9.89260563 13.2112676,9.89260563 C12.8169014,9.89260563 12.471831,9.59683099 12.4225352,9.19014085 C12.028169,9.6584507 11.3257042,9.95422535 10.5492958,9.95422535 C9.60035211,9.95422535 8.47887324,9.31338028 8.47887324,7.98239437 C8.47887324,6.58978873 9.60035211,6.08450704 10.5492958,6.08450704 C11.3380282,6.08450704 12.040493,6.33098592 12.4348592,6.81161972 L12.4348592,5.98591549 C12.4348592,5.38204225 11.9172535,4.98767606 11.1285211,4.98767606 C10.6602113,4.98767606 10.2411972,5.11091549 9.80985915,5.38204225 C9.72359155,5.43133803 9.61267606,5.46830986 9.50176056,5.46830986 C9.18133803,5.46830986 8.91021127,5.1971831 8.91021127,4.86443662 C8.91021127,4.64260563 9.0334507,4.44542254 9.19366197,4.34683099 C9.87147887,3.90316901 10.6232394,3.70598592 11.3996479,3.70598592 Z M11.1778169,8.8943662 C11.6830986,8.8943662 12.1760563,8.72183099 12.4348592,8.37676056 L12.4348592,7.63732394 C12.1760563,7.29225352 11.6830986,7.11971831 11.1778169,7.11971831 C10.5616197,7.11971831 10.056338,7.45246479 10.056338,8.0193662 C10.056338,8.57394366 10.5616197,8.8943662 11.1778169,8.8943662 Z M0.65625,11.125 L13.34375,11.125 C13.7061869,11.125 14,11.4188131 14,11.78125 C14,12.1436869 13.7061869,12.4375 13.34375,12.4375 L0.65625,12.4375 C0.293813133,12.4375 4.43857149e-17,12.1436869 0,11.78125 C-4.43857149e-17,11.4188131 0.293813133,11.125 0.65625,11.125 Z"></path></svg></span>Pre-Training</th><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesMultipleSelect"><path d="M4,3 C4,2.447715 4.447715,2 5,2 L12,2 C12.5523,2 13,2.447716 13,3 C13,3.55228 12.5523,4 12,4 L5,4 C4.447715,4 4,3.55228 4,3 Z M4,7 C4,6.447715 4.447715,6 5,6 L12,6 C12.5523,6 13,6.447716 13,7 C13,7.55228 12.5523,8 12,8 L5,8 C4.447715,8 4,7.55228 4,7 Z M4,11 C4,10.447715 4.447715,10 5,10 L12,10 C12.5523,10 13,10.447716 13,11 C13,11.55228 12.5523,12 12,12 L5,12 C4.447715,12 4,11.55228 4,11 Z M2,4 C1.44771525,4 1,3.55228475 1,3 C1,2.44771525 1.44771525,2 2,2 C2.55228475,2 3,2.44771525 3,3 C3,3.55228475 2.55228475,4 2,4 Z M2,8 C1.44771525,8 1,7.55228475 1,7 C1,6.44771525 1.44771525,6 2,6 C2.55228475,6 3,6.44771525 3,7 C3,7.55228475 2.55228475,8 2,8 Z M2,12 C1.44771525,12 1,11.5522847 1,11 C1,10.4477153 1.44771525,10 2,10 C2.55228475,10 3,10.4477153 3,11 C3,11.5522847 2.55228475,12 2,12 Z"></path></svg></span>Tags</th><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesFile"><path d="M5.94578,14 C4.62416,14 3.38248,13.4963 2.44892,12.585 C1.514641,11.6736 1,10.4639 1,9.17405 C1.00086108,7.88562 1.514641,6.67434 2.44892,5.76378 L7.45612,0.985988 C8.80142,-0.327216 11.1777,-0.332396 12.5354,0.992848 C13.9369,2.36163 13.9369,4.58722 12.5354,5.95418 L8.03046,10.2414 C7.16278,11.0877 5.73682,11.0894 4.86024,10.2345 C3.98394,9.37789 3.98394,7.98769 4.86024,7.1327 L6.60422,5.4317 L7.87576,6.67196 L6.13177,8.37297 C6.01668,8.48539 6.00003,8.61545 6.00003,8.68335 C6.00003,8.75083 6.01668,8.88103 6.13177,8.99429 C6.36197,9.21689 6.53749,9.21689 6.76768,8.99429 L11.2707,4.70622 C11.9645,4.03016 11.9645,2.91757 11.2638,2.23311 C10.5843,1.57007 9.40045,1.57007 8.72077,2.23311 L3.71342,7.0109 C3.12602,7.58406 2.79837,8.35435 2.79837,9.17405 C2.79837,9.99459 3.12602,10.7654 3.72045,11.3446 C4.90947,12.5062 6.98195,12.5062 8.17096,11.3446 L10.41911,9.15165 L11.6906,10.3919 L9.4425,12.585 C8.50808,13.4963 7.2664,14 5.94578,14 Z"></path></svg></span>Files</th><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesText"><path d="M7,4.56818 C7,4.29204 6.77614,4.06818 6.5,4.06818 L0.5,4.06818 C0.223858,4.06818 0,4.29204 0,4.56818 L0,5.61364 C0,5.88978 0.223858,6.11364 0.5,6.11364 L6.5,6.11364 C6.77614,6.11364 7,5.88978 7,5.61364 L7,4.56818 Z M0.5,1 C0.223858,1 0,1.223858 0,1.5 L0,2.54545 C0,2.8216 0.223858,3.04545 0.5,3.04545 L12.5,3.04545 C12.7761,3.04545 13,2.8216 13,2.54545 L13,1.5 C13,1.223858 12.7761,1 12.5,1 L0.5,1 Z M0,8.68182 C0,8.95796 0.223858,9.18182 0.5,9.18182 L11.5,9.18182 C11.7761,9.18182 12,8.95796 12,8.68182 L12,7.63636 C12,7.36022 11.7761,7.13636 11.5,7.13636 L0.5,7.13636 C0.223858,7.13636 0,7.36022 0,7.63636 L0,8.68182 Z M0,11.75 C0,12.0261 0.223858,12.25 0.5,12.25 L9.5,12.25 C9.77614,12.25 10,12.0261 10,11.75 L10,10.70455 C10,10.4284 9.77614,10.20455 9.5,10.20455 L0.5,10.20455 C0.223858,10.20455 0,10.4284 0,10.70455 L0,11.75 Z"></path></svg></span>Summary</th></tr></thead><tbody><tr id="74cc524a-0924-4c89-b096-d014365a4a50"><td class="cell-title"><a href="https://www.notion.so/Word2vec-74cc524a09244c89b096d014365a4a50">Word2vec</a></td><td class="cell-b0_h"></td><td class="cell-Mc\K"></td><td class="cell-}IW&#x27;">- a tool created by Google that produces <mark class="highlight-yellow"><strong>static</strong></mark><mark class="highlight-yellow"> </mark>word embedding
- a shallow NN with each wordâ€™s one-hot embedding as its input and output
- either CBOW or Skip-gram models</td></tr><tr id="0f05d97d-aa90-4e16-9f83-7df136377814"><td class="cell-title"><a href="https://www.notion.so/ELMo-0f05d97daa904e169f837df136377814">ELMo</a></td><td class="cell-b0_h"></td><td class="cell-Mc\K"></td><td class="cell-}IW&#x27;"><mark class="highlight-yellow"><strong>E</strong></mark>mbeddings from <mark class="highlight-yellow"><strong>L</strong></mark>anguage <mark class="highlight-yellow"><strong>Mo</strong></mark>del : <strong><mark class="highlight-yellow">feature-based</mark></strong> approach
- <mark class="highlight-yellow"><strong>dynamic </strong></mark>word embedding with LSTM (context-sensitive)
- structure: word2vec layer + two bidirectional LSTM layers
- connects to other model layers as feature representations </td></tr><tr id="c25e4c72-0b1b-4378-9b47-95c357275e42"><td class="cell-title"><a href="https://www.notion.so/GPT-c25e4c720b1b43789b4795c357275e42">GPT</a></td><td class="cell-b0_h"></td><td class="cell-Mc\K"></td><td class="cell-}IW&#x27;"><strong><mark class="highlight-yellow">G</mark></strong>enerative <strong><mark class="highlight-yellow">P</mark></strong>re-trained <strong><mark class="highlight-yellow">T</mark></strong>ransformer : <strong><mark class="highlight-yellow">fine-tuning</mark></strong> approach
- transformer-<mark class="highlight-yellow"><strong>decoder</strong></mark>-based autoregressive language model 
- generate new embeddings which include the <strong><mark class="highlight-yellow">attention</mark></strong><strong> </strong>information paid to the left
- directly connects the last layer to softmax as the task output layer, then <mark class="highlight-yellow"><strong>fine-tunes</strong></mark></td></tr><tr id="6cfd2575-101b-470d-9a5a-c0075e8d20f1"><td class="cell-title"><a href="https://www.notion.so/BERT-6cfd2575101b470d9a5ac0075e8d20f1">BERT</a></td><td class="cell-b0_h"></td><td class="cell-Mc\K"></td><td class="cell-}IW&#x27;"><strong><mark class="highlight-yellow">B</mark></strong>idirectional <strong><mark class="highlight-yellow">E</mark></strong>ncoder <strong><mark class="highlight-yellow">R</mark></strong>epresentations from <strong><mark class="highlight-yellow">T</mark></strong>ransformers<mark class="highlight-yellow_background">
</mark>- transformer-encoder-based autoencoder language model
- Masked Language Model
- input: WordPiece embedding + position embedding + segment embedding</td></tr><tr id="0bf9ab91-d86d-4492-9f79-1f6ce9fc69c3"><td class="cell-title"><a href="https://www.notion.so/XLNet-0bf9ab91d86d44929f791f6ce9fc69c3">XLNet</a></td><td class="cell-b0_h"></td><td class="cell-Mc\K"></td><td class="cell-}IW&#x27;">- integrates GPT and BERT
- autoregressive model and DAE (denoise autoencoder) model
??</td></tr></tbody></table></div><p id="c4b91918-b392-4115-913d-32d5e45c6e54" class="">ğŸŒ <mark class="highlight-gray"><em>Reference: </em></mark><a href="https://medium.com/ai%C2%B3-theory-practice-business/what-is-pre-training-in-nlp-introducing-5-key-technologies-455c54933054"><mark class="highlight-gray"><em>What is Pre-Training in NLP? Introducing 5 Key Technologies, Medium</em></mark></a></p><h2 id="98064190-8128-426c-a00b-05ce5c632a53" class="">Attention Mechanisms</h2><div id="5785ef68-1273-4ac2-885a-7bf8b765dc74" class="column-list"><div id="51a7b1a0-364d-4af2-a185-9913cb34c344" style="width:75%" class="column"><ul id="cce8a221-da98-4ccc-94df-556610c3fbf5" class="bulleted-list"><li>(MOTIVATION) Recurrent Neural Networks are known to have problems dealing with such <em><strong><mark class="highlight-yellow">long-range dependencies</mark></strong></em>. In theory, architectures likeÂ LSTMsÂ should be able to deal with this, but in practice long-range dependencies are still problematic</li></ul><ul id="e0951213-1eea-463f-a4e7-40ca7e074837" class="bulleted-list"><li>Attention mechanisms allow the decoder to <em><strong><mark class="highlight-yellow">â€œattendâ€ to different parts of the source sentence</mark></strong></em> at each step of the output generation</li></ul><ul id="144e746e-ed6c-4eeb-bff6-b95987ef4fd6" class="bulleted-list"><li>Each decoder output word now depends on aÂ <em><mark class="highlight-yellow"><strong>weighted combination </strong></mark></em>(learning parameters)<em><mark class="highlight-yellow"><strong> </strong></mark></em>of all the input states, not just the last state</li></ul></div><div id="e30a8fa2-82b5-4090-a95b-6ae075820d25" style="width:25.000000000000043%" class="column"><figure id="a2722d8a-f507-48a3-8745-50e92e103949" class="image"><a href="Understanding%20BERT%20for%20NLP%20Starters%20f940e4a655a94ff1a1d212eb9ada87fe/Untitled%201.png"><img style="width:240px" src="Understanding%20BERT%20for%20NLP%20Starters%20f940e4a655a94ff1a1d212eb9ada87fe/Untitled%201.png"/></a></figure><p id="af354a92-09d8-41b1-8b56-2b309eef5848" class="">
</p></div></div><p id="5f6efd6c-6f74-45e6-8d33-2c9992a616dd" class="">ğŸŒ <mark class="highlight-gray"><em>Reference: </em></mark><mark class="highlight-gray"><em><a href="https://towardsdatascience.com/attention-models-in-nlp-a-quick-introduction-2593c1fe35eb">Attention models in NLP a quick introduction, Towards Data Science</a></em></mark></p><h2 id="e3ce80ae-e3c2-4505-95f6-20d4ffe26543" class="">Transformer</h2><ul id="aeef22a1-0c79-43b5-be87-6223f64b207e" class="bulleted-list"><li>A novel neural network architecture based on a <mark class="highlight-yellow"><em><strong>self-attention mechanism</strong></em></mark> <ul id="678662ae-aea2-4609-91bc-841e280a229e" class="bulleted-list"><li>Performs only a small, constant number of steps (chosen empirically)</li></ul><ul id="2b260740-6c11-4535-bd6c-6c3339b883c6" class="bulleted-list"><li>In each step, a self-attention mechanism <mark class="highlight-teal"><em><strong>directly models relationships between all words in a sentence</strong></em></mark>, regardless of their respective position</li></ul><ul id="b5d16f91-6fac-4d3f-a8ce-c352b3e01799" class="bulleted-list"><li><mark class="highlight-teal"><em><strong>Attention scores</strong></em></mark> is computed to determine how much each of the other words should contribute to the next representation of the target word</li></ul><ul id="58a4e65a-6090-448d-8ee5-7cf0ce1a81ea" class="bulleted-list"><li>The decoder attends not only to the other previously generated words, but <mark class="highlight-teal"><em><strong>also to the final representations generated by the encoder</strong></em></mark></li></ul></li></ul><ul id="62dd91a7-64b6-40de-8970-64a40b000b25" class="bulleted-list"><li>Advantages: computational performance, higher accuracy, <mark class="highlight-yellow"><em><strong>visualization</strong></em></mark></li></ul><figure id="bf9d46bc-4bce-448b-b2a8-826613c7e3bd" class="image"><a href="Understanding%20BERT%20for%20NLP%20Starters%20f940e4a655a94ff1a1d212eb9ada87fe/Untitled%202.png"><img style="width:640px" src="Understanding%20BERT%20for%20NLP%20Starters%20f940e4a655a94ff1a1d212eb9ada87fe/Untitled%202.png"/></a></figure><p id="190f12b9-6560-40c0-a323-05306fa88bc0" class="">ğŸŒ <mark class="highlight-gray"><em>Reference: </em></mark><mark class="highlight-gray"><em><a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">Google AI Blog</a></em></mark></p><h2 id="fae34467-8d25-46f5-ab21-12108c21b7e8" class=""><mark class="highlight-orange">BERT: Bidirectional Encoder Representations from Transformers</mark></h2><ul id="63def704-a296-4dd4-a600-f60fa56fe912" class="bulleted-list"><li>model architecture: <mark class="highlight-yellow"><em><strong>multi-layer bidirectional Transformer encoder
</strong></em></mark>- nearly identical to GPT in terms of model architecture apart from the attention masking</li></ul><ul id="fac02ed7-8249-44dd-a931-e1acfb88e5a4" class="bulleted-list"><li>input representation:<figure id="2b953545-43ed-48f7-8308-faa88cbb8567" class="image"><a href="Understanding%20BERT%20for%20NLP%20Starters%20f940e4a655a94ff1a1d212eb9ada87fe/Untitled%203.png"><img style="width:1347px" src="Understanding%20BERT%20for%20NLP%20Starters%20f940e4a655a94ff1a1d212eb9ada87fe/Untitled%203.png"/></a></figure></li></ul><h3 id="7809a4db-4e11-47e7-b53f-088a3983cb08" class="">Step 1. Pre-training: using two unsupervised tasks</h3><ul id="44bc4894-4bb4-40c3-9e9d-301d23819ce3" class="bulleted-list"><li><mark class="highlight-yellow"><strong>Task #1: Masked LM</strong></mark>
- mask some percentage of the input tokens at random and then predict them</li></ul><ul id="7e1d0c20-c580-4bae-847d-c85023d80e66" class="bulleted-list"><li><mark class="highlight-yellow"><strong>Task #2: Next Sentence Prediction (NSP)</strong></mark>
- Sentence B = 50% IsNext (actual sentence after A) + 50% NotNext (random sentence)</li></ul><h3 id="284d88a4-1198-4670-b900-4023501302ff" class="">Step 2. Fine-tuning</h3><ul id="8337f24e-8e95-4fbf-86d7-aae94f9ab6b0" class="bulleted-list"><li>first initialize with the pre-trained parameters 
â†’ fine-tune using labeled data from the downstream tasks </li></ul><ul id="a07926d8-f1d2-4967-9dc6-16cdb99f49ab" class="bulleted-list"><li>minimal difference between the pre-trained architecture and the final downstream architecture</li></ul><h3 id="7a6e07b4-08b7-4171-a846-77ad37a4f155" class="">Performance</h3><ul id="102e0223-ba97-4bb9-b7b5-43b267d55695" class="bulleted-list"><li>(GLUE) Both BERT BASE/LARGE outperform all systems on all tasks by a substantial margin</li></ul><ul id="ad45ef54-3a1f-4d7d-92e3-9dab7024fbd4" class="bulleted-list"><li>BERT LARGE significantly outperforms BERT BASE across all tasks</li></ul><p id="77aa3969-816c-404c-99d6-eb68184515ba" class="">
</p></div></article></body></html>
